{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Steps Delayed Mode\n",
    "- author: J. Klymak\n",
    "- deployment: dfo-walle652-20191209\n",
    "- glider: dfo-walle652\n",
    "- description: Line P mission, starting from west coast and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data from card offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    !mkdir raw\n",
    "    !rsync -av ../../../card_offloads/dfo-walle652/dfo-walle652_20190718/nav_20191113/LOGS/*.DBD raw/\n",
    "    !rsync -av ../../../card_offloads/dfo-walle652/dfo-walle652_20190718/science_20191113/LOGS/*.EBD raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the processing\n",
    "\n",
    "This largely gets driven by `process_deployment.py`.  Note that some directories need to be made, the `cac` files need to be copied from t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seawater \n",
    "%matplotlib notebook\n",
    "import matplotlib.units as munits\n",
    "import pyglider\n",
    "import pyglider.ncprocess as ncprocess\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "converter = mdates.ConciseDateConverter()\n",
    "munits.registry[np.datetime64] = converter\n",
    "\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "import pyglider.ncprocess as ncprocess\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_name = 'dfo-walle652-20191209'\n",
    "deploy_prefix = '../../../deployments/dfo-walle652/dfo-walle652-20191209/'\n",
    "!mkdir figs\n",
    "def get_timeseries(level='L0'):\n",
    "    return xr.open_dataset(f'{deploy_prefix}/{level}-timeseries/{deploy_name}_{level}.nc')\n",
    "\n",
    "def get_gridfile(level='L0'):\n",
    "    return xr.open_dataset(f'{deploy_prefix}/{level}-gridfiles/{deploy_name}_grid.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Check\n",
    "\n",
    "Check that the profile computing algorithm worked well.\n",
    "\n",
    "As can be seen in the plot below, the profiles are being properly associated with up and down casts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profiles(fname, plotname):\n",
    "\n",
    "    with xr.open_dataset(fname) as ds0:\n",
    "        N = len(ds0.time)\n",
    "        print(N)\n",
    "        todo = [slice(0, 70000), slice(int(N/2), int(N/2)+70000), slice(-70000, -1),]\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=3, constrained_layout=True, figsize=(12, 6))\n",
    "\n",
    "        for nn, td in enumerate(todo):\n",
    "            ds = ds0.isel(time=td)\n",
    "            axs[0, nn].plot(ds.time, ds.pressure, '.', markersize=1)\n",
    "            axs[0, nn].set_ylim([1000, 0])\n",
    "            if nn==0:\n",
    "                axs[0, nn].set_ylabel('P [dbar]')\n",
    "            else:\n",
    "                axs[0, nn].set_yticklabels('')\n",
    "            axs[0, nn].set_xticklabels('')\n",
    "   \n",
    "            axs[1, nn].plot(ds.time, ds.profile_index, '.', markersize=1)\n",
    "            if nn==0:\n",
    "                axs[1, nn].set_ylabel('Profile')\n",
    "            else:\n",
    "                axs[1, nn].set_yticklabels('')\n",
    "            axs[1, nn].set_xticklabels('')\n",
    "\n",
    "            axs[2, nn].plot(ds.time, ds.profile_direction, '.', markersize=1)\n",
    "            if nn==0:\n",
    "                axs[2, nn].set_ylabel('Direction')\n",
    "            else:\n",
    "                axs[2, nn].set_yticklabels('')\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "    fig.savefig(plotname)\n",
    "            \n",
    "plot_profiles(f'{deploy_prefix}/L0-timeseries/{deploy_name}_L0.nc', 'figs/ProfileCheck.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for suspect salinity profiles\n",
    "\n",
    "Biology often gets caught in the conductivity cell and leads to large blips in the salinity (and sometimes the temperature).  This simple check looks for profiles that are obviously contaminated by this to exclude from analysis below.  More stringent checks will be applied later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with get_gridfile() as ds:\n",
    "    fig, ax = plt.subplots()\n",
    "    Tmean = ds['temperature'].mean(dim='time')\n",
    "\n",
    "Tmean = Tmean.sortby(Tmean, ascending=True).where(np.isfinite(Tmean), drop=True)\n",
    "\n",
    "def get_salinity_grid(ts):\n",
    "    ts = ts.where(np.isfinite(ts.salinity), drop=True)\n",
    "    \n",
    "    # bin the data:\n",
    "    tbins = Tmean.values[::7]\n",
    "    profile_bins = np.unique(ts['profile_index'])\n",
    "    profile_bins = np.hstack([profile_bins, profile_bins[-1]+1])\n",
    "    direction = profile_bins * 0\n",
    "    for n, i in enumerate(profile_bins):\n",
    "        ind = np.where(ts['profile_index'] == i)[0]\n",
    "        direction[n] = np.median(ts.profile_direction[ind])\n",
    "\n",
    "        \n",
    "    salin, xx, yy, binn = stats.binned_statistic_2d(\n",
    "                        ts['temperature'].values,\n",
    "                        ts['profile_index'].values,\n",
    "                        values=ts['salinity'].values, statistic='mean',\n",
    "                        bins=[tbins, profile_bins])\n",
    "    sal = xr.Dataset({'profiles': profile_bins[:-1], 'temperature': tbins[:-1]+np.diff(tbins)/2, 'salinity':(('temperature', 'profiles'), salin)})\n",
    "    sal['dS'] = np.abs(sal.salinity - sal.salinity.mean(dim='profiles'))\n",
    "    sal['dS'].plot(vmin=0, vmax=3, cmap='hot')\n",
    "    sal['salinityClean'] = sal.salinity.where( sal.dS < 4 * sal.salinity[:, :1000].std(dim='profiles'))\n",
    "    sal['NgoodSal'] = (sal.salinity / sal.salinity).sum(dim='temperature')\n",
    "\n",
    "    sal['NgoodSalClean'] = (sal.salinityClean / sal.salinityClean).sum(dim='temperature')\n",
    "    sal['profile_direction'] = direction\n",
    "\n",
    "    sal['bad'] = (sal.NgoodSal - sal.NgoodSalClean) / sal.NgoodSal\n",
    "    Bad = sal.bad + 0 * sal.salinityClean\n",
    "    \n",
    "    sal['salinityClean'] = sal.salinityClean.where(Bad<0.025) \n",
    "    \n",
    "    return sal\n",
    "\n",
    "with get_timeseries() as ts:\n",
    "    # ts = ts.sel(time=slice(np.datetime64('2019-09-25'), np.datetime64('2019-09-27T03:00:00')))\n",
    "    sal = get_salinity_grid(ts)\n",
    "    \n",
    "sal.to_netcdf('SalinityGrid.nc')\n",
    "bad_profiles = sal.profiles.where(sal.bad > 0.05, drop=True)\n",
    "print('Bad:', bad_profiles.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the below plots, where salinity is plotted into temperature space, we see that salinities that are about 4 standard deviations from the mean are good candidates to flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset('SalinityGrid.nc') as sal:\n",
    "    fig, ax = plt.subplots()\n",
    "    sal.salinity.plot(vmin=30)\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    sal.salinity.std(dim='profiles').plot(ax=ax[0])\n",
    "    sal.salinity.mean(dim='profiles').plot(ax=ax[1])\n",
    "    fig, ax = plt.subplots()\n",
    "    (sal['salinity']-sal.salinity.mean(dim='profiles')).plot(vmin=-2, vmax=2, cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T/C offset\n",
    "\n",
    "Check if there should be a time offset applied to the conductivity cell to match the temperature.\n",
    "\n",
    "From the plot below, we see that there is a slight up/down asymmetry, with the profiles saltier on the way down (profile 100 and 102) than on the way up, particularly right near the seasonal thermocline.  This really isn't too bad, but lets try and fix.\n",
    "\n",
    "This CTD is pumped, however, so the lags are nowhere near as bad as other glider CTDs.  The evidence below is that perhaps a half-second delay will improve the match near the surface, but adding that delay makes the profile more variable at deeper depths, so the advantage is quite minimal.  Note the tightness of the deep T/S curves, so overall this CTD is doing quite a good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_timeseries() as ds0:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for td in range(220, 224):\n",
    "        \n",
    "        ds = ds0.where(ds0['profile_index']==td, drop=True)\n",
    "        ax.plot(ds.salinity, ds.temperature, '.', markersize=1, label=f'Cast: {td}, {ds.profile_direction[10].values}' )\n",
    "        ax.set_xlabel('Salinity [psu]')\n",
    "        ax.set_ylabel('Temperature $[^oC]$')\n",
    "        ax.set_title(f'{deploy_name}', loc='left')\n",
    "        ax.grid(True)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue is easily seen in the temperature-gridded salinity shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset('SalinityGrid.nc') as sal:\n",
    "    print(sal)\n",
    "    fig, ax = plt.subplots(2, 1, sharex=True, constrained_layout=True)\n",
    "    ax[0].plot(sal.profiles, sal.profile_direction[:-1])\n",
    "    ax[0].set_ylim([-1.2, 1.2])\n",
    "    ax[0].set_ylabel('Profile Dir')\n",
    "    sal.salinityClean.plot(ax=ax[1], vmin=32, vmax=33)\n",
    "    ax[0].set_xlim(400, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive filter on temperature\n",
    "\n",
    "Following Garauetal11a, Morison et al 1994:\n",
    "\n",
    "A correction to the temperature is made with the following filter: \n",
    "\n",
    "`dTn(n) = -b dTn(n-1) + a[T(n) - T(n-1)]`\n",
    "\n",
    "or in terms of a `lfilter` with `a[0]=1`\n",
    "\n",
    "`dTn(n) = a[1] * dTn[n-1] + b[0]*T[n] + b[1]*T[n-1]`\n",
    "\n",
    "and `b[1] = -b[0]`.\n",
    "\n",
    "They say $b_0 = \\frac{4 f_n \\alpha \\tau}{1 + 4 f_n \\tau}$, where $fn$ is the Nyquist frequency (=0.25 for seabird GPCTD), $\\alpha$ is the amplitude of the error, and $\\tau$ is a timeconstant.  $\\alpha$ and $\\tau$ are determined by minimizing the difference between up and down casts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " dtime = diff(time_val);\n",
    "% sampling_freq = 1 ./ dtime;\n",
    "% nyquist_freq = 0.5 * sampling_freq;\n",
    "% coefa = alpha .* (4 * nyquist_freq .* tau) ./ (1 + 4 * nyquist_freq .* tau);\n",
    "% coefb = 1 - 2  * (4 * nyquist_freq .* tau) ./ (1 + 4 * nyquist_freq .* tau);\n",
    "% coefa = 2 * alpha ./ (2 + dtime .* beta); % from SBE Data Processing.\n",
    "% coefb = 1 - 2 .* coefa ./ alpha;          \n",
    "coefa = 2 * alpha ./ (2 + dtime ./ tau);  % same using tau instead of beta.\n",
    "coefb = 1 - 4 ./ (2 + dtime ./ tau);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of running this procedure on a snapshot of data, and the routine for doing the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_TS_diff(alphatau, ts, fn, reterr=True):\n",
    "\n",
    "    alpha, tau = alphatau\n",
    "    alpha = alpha / 1e3\n",
    "    # print(alpha, tau)\n",
    "\n",
    "    coefa = 4 * fn * alpha * tau / (1 + 4 * fn * tau)\n",
    "    if coefa == 0:\n",
    "        coefb = 0\n",
    "    else:\n",
    "        coefb = 1 - 2 * coefa / alpha\n",
    "    b = np.array([1, -1]) * coefa\n",
    "    a = np.array([1, coefb])\n",
    "\n",
    "\n",
    "    ts = ts.where(np.isfinite(ts.temperature+ts.conductivity), drop=True)\n",
    "\n",
    "    x0 = ts.temperature.values\n",
    "    x0  = signal.lfilter(b, a, ts.temperature.values)\n",
    "    \n",
    "    ts['temperature'] =  ts.temperature - x0\n",
    "    ts['salinity'].values = seawater.eos80.salt(ts.conductivity / seawater.constants.c3515 * 10, \n",
    "                                         ts.temperature, ts.pressure)\n",
    "    ts = ts.where(np.isfinite(ts.salinity), drop=True)\n",
    "    \n",
    "    # bin the data:\n",
    "    tbins = Tmean.values[::7]\n",
    "    tbins = tbins[tbins<16]\n",
    "    tbins = tbins[tbins > 6]\n",
    "    profile_bins = np.unique(ts['profile_index'])\n",
    "    direction = profile_bins * 0\n",
    "    for n, i in enumerate(profile_bins):\n",
    "        ind = np.where(ts['profile_index'] == i)[0]\n",
    "        direction[n] = np.median(ts.profile_direction[ind])\n",
    "    direction = direction[1:-1]\n",
    "    profile_bins = profile_bins[1:]\n",
    "\n",
    "    sal, xx, yy, binn = stats.binned_statistic_2d(\n",
    "                        ts['temperature'].values,\n",
    "                        ts['profile_index'].values,\n",
    "                        values=ts['salinity'].values, statistic='mean',\n",
    "                        bins=[tbins, profile_bins])\n",
    "    ind = np.where(np.abs(np.diff(direction)))  \n",
    "    err = np.nansum(np.nansum(np.diff(sal, axis=1)[:, ind]**2, axis=0))\n",
    "    if reterr:\n",
    "        return err\n",
    "    else:\n",
    "        return ts, sal\n",
    "\n",
    "\n",
    "with get_timeseries() as ts:\n",
    "    t0 = np.datetime64('2019-08-01')\n",
    "    ts = ts.sel(time=slice(t0, t0 + np.timedelta64(3, 'D')))\n",
    "    ts = ts.where(np.isfinite(ts.temperature+ts.conductivity), drop=True, )\n",
    "    for bad in bad_profiles:\n",
    "        ts = ts.where(~(ts.profile_index==bad), drop=True)\n",
    "    fig, ax = plt.subplots()\n",
    "    ind = np.where(ts.profile_direction.values== 1)[0]\n",
    "    ax.plot(ts.salinity[ind], ts.temperature[ind], '.', markersize=1)\n",
    "\n",
    "    ind = np.where(ts.profile_direction.values == -1)[0]\n",
    "    ax.plot(ts.salinity[ind], ts.temperature[ind], 'r.', markersize=1)\n",
    "\n",
    "    import scipy.optimize as optimize\n",
    "\n",
    "    bnds = ((0.0001, 200), (0.0001, 150))\n",
    "    if 1:\n",
    "        res = optimize.basinhopping(get_TS_diff, (20, 20), T=0.5, stepsize=2, \n",
    "                                    minimizer_kwargs={'tol':1e-4, 'bounds':bnds, 'args':(ts, 0.25)})\n",
    "    \n",
    "    tau = res.x[1]\n",
    "    alpha = res.x[0] / 1e3\n",
    "    \n",
    "    newts, sal = get_TS_diff((alpha, tau), ts, 0.25, reterr=False)\n",
    "    \n",
    "    ind = np.where(newts.profile_direction.values== 1)[0]\n",
    "    ax.plot(newts.salinity[ind]+3, newts.temperature[ind], 'g.', markersize=1)\n",
    "\n",
    "    ind = np.where(newts.profile_direction.values==-1)[0]\n",
    "    ax.plot(newts.salinity[ind]+3, newts.temperature[ind], 'r.', markersize=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[1].pcolormesh(sal)\n",
    "    _, salold = get_TS_diff((0, 0), ts, 0.25, reterr=False)\n",
    "    ax[0].pcolormesh(salold)\n",
    "\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine IIR filter co-eficitients\n",
    "\n",
    "Using this minimizing procedure, we apply this in three day blocks.  The region of a \"good\" fit is quite broad, so here we choose $\\tau = 20s$ and $\\alpha = 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# WARNING: SLOW!!\n",
    "\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "    \n",
    "with get_timeseries() as ts0:\n",
    "    times = np.arange(ts0.time.values[0], ts0.time.values[-1], 7, dtype='datetime64[D]')\n",
    "    ts0 = ts0.where(np.isfinite(ts0.temperature+ts0.conductivity), drop=True, )\n",
    "    print('Dropped')\n",
    "    for bad in bad_profiles:\n",
    "        ts0 = ts0.where(~(ts0.profile_index==bad))\n",
    "    print('Bad dropped')\n",
    "    alphas = np.zeros(len(times)) + np.NaN\n",
    "    taus = np.zeros(len(times)) + np.NaN\n",
    "    \n",
    "    at0 = (2, 20)\n",
    "    for nn, t0 in enumerate(times):\n",
    "        ts = ts0.sel(time=slice(t0, t0 + np.timedelta64(3, 'D')))\n",
    "        bnds = ((0.0001, 200), (0.0001, 150))\n",
    "        res = optimize.basinhopping(get_TS_diff, at0, minimizer_kwargs={'tol':1e-4, 'args': (ts, 0.25), 'bounds':bnds})\n",
    "        print(res)\n",
    "        if res.lowest_optimization_result.success:\n",
    "            alphas[nn] = res.x[0] / 10^3\n",
    "            taus[nn] = res.x[1]\n",
    "            at0 = (np.nanmean(alphas) * 100, np.nanmean(taus))\n",
    "        update_progress(nn / len(times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1)\n",
    "axs[0].plot(times, taus)\n",
    "axs[0].set_ylabel(r'$\\tau\\ [s]$')\n",
    "axs[0].set_title(f'{deploy_name}: Up-down T/S cast matching', loc='left')\n",
    "axs[1].plot(times, alphas)\n",
    "axs[1].set_ylabel(r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1)\n",
    "axs[0].plot(times, taus)\n",
    "axs[0].set_ylabel(r'$\\tau\\ [s]$')\n",
    "axs[0].set_title(f'{deploy_name}: Up-down T/S cast matching', loc='left')\n",
    "axs[1].plot(times, alphas)\n",
    "axs[1].set_ylabel(r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_salinity(ts, tau=12, alpha=0.03, fn=0.25):\n",
    "    \"\"\"\n",
    "    Apply the Lueck 1990, Morrison et al 1994 salinity correction to the the data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    ts : xarray\n",
    "        timeseries data, must have 'temperature', 'salinity', 'conductivity', and \n",
    "        'pressure' variables.\n",
    "    \n",
    "    tau : float\n",
    "        time constant, seconds for filter to apply to temperature data used to \n",
    "        calculate the salinity\n",
    "        \n",
    "    alpha : float\n",
    "        fraction of the signal to correct.\n",
    "\n",
    "    Note that ``tau`` and ``alpha`` are determined empirically \n",
    "    (i.e. see write up in ``process_delayed_20190718``).\n",
    "    \"\"\"\n",
    "    \n",
    "    coefa = 4 * fn * alpha * tau / (1 + 4 * fn * tau)\n",
    "    if coefa == 0:\n",
    "        coefb = 0\n",
    "    else:\n",
    "        coefb = 1 - 2 * coefa / alpha\n",
    "    b = np.array([1, -1]) * coefa\n",
    "    a = np.array([1, coefb])\n",
    "\n",
    "    good = np.where(np.isfinite(ts.temperature))\n",
    "    print('done good')\n",
    "    x0 = ts.temperature.values[good]\n",
    "    x0  = signal.lfilter(b, a, x0)\n",
    "    print('Done filter')\n",
    "    t =  ts.temperature.values[good] - x0\n",
    "    ts['salinity'].values[good] = seawater.eos80.salt(ts.conductivity.values[good] / seawater.constants.c3515 * 10, \n",
    "                                         t, ts.pressure.values[good])\n",
    "    ts['salinity'].attrs['comment'] = ('Salinity, calculated from temperature filtered to minimize '\n",
    "                                      'up/down profile differences, following Morison et al 1994. '\n",
    "                                      'Constants calculated from cruise means and checked in notebook.')\n",
    "    ts['salinity'].attrs['temp_corrections_factors'] = f'alpha={alpha}, tau={tau}'\n",
    "    ts['salinity'].attrs['method'] += ' ncprocess.correct_salinity'\n",
    "    \n",
    "    ts.attrs['data_mode'] = 'M'\n",
    "    ts.attrs['processing_level'] = 'Mixed: salinity thermal lag corrections applied'\n",
    "    \n",
    "    return ts\n",
    "    \n",
    "    \n",
    "with get_timeseries() as ts:\n",
    "    ts = correct_salinity(ts, tau=20, alpha = 0.02, fn=0.25)\n",
    "    # !mkdir /Users/jklymak/gliderdata/deployments/dfo-walle652/dfo-walle652-20190718/L1-timeseries/\n",
    "    ts.to_netcdf(f'{deploy_prefix}/L1-timeseries/{deploy_name}_L1.nc')\n",
    "    print(f'Saved {deploy_prefix}/L1-timeseries/{deploy_name}_L1.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make grid\n",
    "\n",
    "Making the level-1 grid with the new salinity in it, and plotting.  There is still *some* residual up-down assymetry, but it is substantially less than before, and this is really a very strong thermocline right here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncprocess.make_L2_gridfiles(f'{deploy_prefix}/L1-timeseries/{deploy_name}_L1.nc', \n",
    "                            f'{deploy_prefix}/L1-gridfiles/', './deployment.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "with get_gridfile(level='L1') as ds:\n",
    "    fig, axs = plt.subplots(3, 1, sharex=True, sharey=True, constrained_layout=True, figsize=(6, 7))\n",
    "    ds.salinity.plot(ax=axs[1], vmin=32, vmax=33)\n",
    "    axs[1].set_title('Correction ' + ds.salinity.attrs['temp_corrections_factors'])\n",
    "\n",
    "    with get_gridfile(level='L0') as ds:\n",
    "        ds.salinity.plot(ax=axs[0], vmin=32, vmax=33)\n",
    "        axs[0].set_title('No correction')\n",
    "    axs[0].set_xlabel('')\n",
    "    \n",
    "    ds.temperature.plot(ax=axs[2], vmin=6, vmax=15.5)\n",
    "    \n",
    "    #axs[0].set_xlim(np.datetime64('2019-10-03'), np.datetime64('2019-10-09'))\n",
    "    axs[0].set_ylim(100, 0)\n",
    "# ## Screen for outliers (T and S)\n",
    "#\n",
    "# This is really much more easily done on the gridded data, but should feed back to the time series data as well.  As seen above in the first-pass at this, there isn't that much data that is bad, and can almost be edited by hand.  Its tempting to also just remove whole profile, which is a pretty easy thing to do.  Are there many profiles where the salps didn't mess up most of the profile?\n",
    "#\n",
    "\n",
    "with get_gridfile(level='L1') as ds:\n",
    "    t = ds.salinity - ds.salinity.mean(dim='time')\n",
    "    fig, axs = plt.subplots(2, 1, constrained_layout=True, sharex=True, gridspec_kw={'height_ratios':[1 , 0.5]})\n",
    "    t.plot(ax=axs[0], vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "    axs[0].set_ylim(1000, 0)\n",
    "    t.mean(dim='depth').plot(ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screening bad data\n",
    "\n",
    "This is probably not much different than above, but it would be nice to automate it somewhat.  What makes a bad temperature or salinity?  Very much out of the natural range.  It is also nice to feed this back into the time series, so that will be a bit of an issue, but starting on density will be quickest...\n",
    "\n",
    "  - Remove outliers\n",
    "  - Screen bad T/S\n",
    "  - Find salinity anomalies\n",
    "\n",
    "### Grid onto potential density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with get_gridfile() as ds:\n",
    "    Rmean = ds['potential_density'].mean(dim='time')\n",
    "    print(ds.potential_density.max())\n",
    "    print(ds.potential_density.min())\n",
    "    \n",
    "means = xr.Dataset()\n",
    "Rmean = Rmean.sortby(Rmean, ascending=True).where(np.isfinite(Rmean), drop=True)\n",
    "\n",
    "Rbins = Rmean[::1]\n",
    "Rbins = np.hstack([1020, Rbins, 1027.4])\n",
    "# print(Rbins, type(Rbins))\n",
    "\n",
    "with get_timeseries(level='L1') as ts:\n",
    "    good = np.where(np.isfinite(ts.salinity + ts.potential_density + ts.temperature))[0]\n",
    "    Smean, _, _ = stats.binned_statistic(ts.potential_density.values[good], \n",
    "                                         ts.salinity.values[good], statistic='mean', bins=Rbins)\n",
    "\n",
    "    Tmean, _, _ = stats.binned_statistic(ts.potential_density.values[good], ts.temperature.values[good], \n",
    "                                         statistic='mean', bins=Rbins)\n",
    "    \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "Rmean.plot(ax=ax[0])\n",
    "\n",
    "ax[1].plot(Smean)\n",
    "\n",
    "ax[2].plot(Tmean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make salinity anomaly along isopycnals\n",
    "\n",
    "We will compute this as $\\delta S(\\sigma_{\\theta}) = S(\\sigma_{\\theta}) - S_0 (\\sigma_{\\theta})$.  We will then high-pass this to remove regional differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!atom ./bad_salinity.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_gridfile(level='L1') as ds:\n",
    "    sanom = ds.salinity - np.interp(ds.potential_density, Rbins[:-1] + np.diff(Rbins) / 2, Smean)\n",
    "    sanom = sanom.fillna(0)\n",
    "    s0 = sanom.rolling(time=61, center=True).mean()\n",
    "    ds['salinity_anomaly'] = sanom - s0\n",
    "    fig, ax = plt.subplots(3, 1, sharex=True, gridspec_kw={'height_ratios':[1, 1, 0.5]}, constrained_layout=True)\n",
    "    ax[0].pcolormesh(ds.profile, ds.depth,ds.salinity)\n",
    "    ax[0].set_ylim(1000, 0)\n",
    "\n",
    "    ax[1].pcolormesh(ds.profile, ds.depth, ds.salinity_anomaly, vmin=-0.2, vmax=0.2, cmap='RdBu_r')\n",
    "    ax[1].set_ylim(1000, 0)\n",
    "    \n",
    "    ax[2].plot(ds.profile, np.nanmax(np.abs(ds.salinity_anomaly.values), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the plot above to make `bad_salinity.csv` and then that is readily read in with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import datetime\n",
    "os.system(f'!mkdir {deploy_prefix}/L2-gridfiles/')\n",
    "import pandas as pd \n",
    "\n",
    "bad_salinity =  pd.read_csv('bad_salinity.csv')\n",
    "with get_gridfile(level='L1') as ds:\n",
    "    for index, bad in bad_salinity.iterrows():   \n",
    "        ind = np.where(ds.profile == bad.profile)[0]\n",
    "        badd = np.where((ds.depth > bad.pstart) & (ds.depth<bad.pstop))[0]\n",
    "        ds.salinity[badd, ind] = np.NaN\n",
    "    fig, ax = plt.subplots()\n",
    "    ds.salinity.plot()\n",
    "    ds.salinity.attrs['comment'] += ';\\nSalinity spikes removed manually (bad_salinity.csv);'\n",
    "    ds.attrs['date_modified'] = datetime.datetime.now().isoformat()\n",
    "    ds.attrs['processing_level'] += ';\\nSalinity spikes removed (conductivity not changed); '\n",
    "    \n",
    "    ds.to_netcdf(f'{deploy_prefix}/L2-gridfiles/{deploy_name}_grid.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make L2 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir {deploy_prefix}/L2-timeseries/')\n",
    "\n",
    "\n",
    "with get_gridfile(level='L1') as ts:\n",
    "    for index, bad in bad_salinity.iterrows():   \n",
    "        ind = np.where((ts.profile_index == bad.profile) &\n",
    "                       (ts.pressure > bad.pstart) &\n",
    "                       (ts.pressure < bad.pstop))[0]\n",
    "        ts.salinity[ind] = np.NaN\n",
    "    ts.salinity.attrs['comment'] += ';\\nSalinity spikes removed manually (bad_salinity.csv);'\n",
    "    ts.attrs['date_modified'] = datetime.datetime.now().isoformat()\n",
    "    ts.attrs['processing_level'] += ';\\nSalinity spikes removed (conductivity not changed); '\n",
    "    \n",
    "    ts.to_netcdf(f'{deploy_prefix}/L2-timeseries/{deploy_name}_L2.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
